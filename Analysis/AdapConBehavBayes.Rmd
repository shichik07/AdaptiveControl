---
title: "Adaptive Control Behavior Bayesian Analysis"
author: "Julius Kricheldorff"
date: "3/30/2022"
output: html_document
---

# Analysis of the behavioral data using Bayesian mixed-effect models

Load packages
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(brms)
library(ggplot2)
library(dplyr)
library(tidyr)
library(purrr)
library(bayesplot)
library(tictoc)
library(extraDistr) # more distributions
library(haven)
library(purrr)
library(hypr)
```

## Brief Task and hypothesis description

The task was constructed as follows:

### Rationale

Participants saw two numbers on the screen and were tasked to judge the numerically larger number. Numbers were varying in physical display size on screen. The larger numerically larger number could either be larger- (**congruent**) or smaller (**incongruent**) than the other number. The difference in reaction time between answering correctly to congruent and incongruent pairings is a variation of the classical Stroop effect in Psychology, thought to reflect the recruitment of **cognitive control** functions.

We use this paradigm to test **adaptive control** effects in healthy participants and participants with Parkinson's disease. Adaptive control is the ability to regulate the amount of cognitive control that is employed according to context. For instance in a context were you would expect high conflict cognitive control resources would need to be higher compared to a context were little conflict is expected. There are two modes by which this can happen - **proactively** and **reactively**. The exact details would be a bit too much information - but these are the target of our investigation. 

### Task Manipulation

*Basic task design*: Participants completed a numerical Stroop task judging numerical size with congruence manipulated as described above. The task had in total about 1100 repetitions. Repetitions were separated into blocks (in total 8), so participants did approximately 120 repetitions an then took a little break. To manipulate **proactive control** two of the blocks had a higher proportion of congruent trials than incongruent trials and two of the blocks had a higher proportion of congruent trials than incongruent trials. Here specifically the interaction between congruence and Block_type is how we operationalize the effect of proactive control on reaction times - That is in highly incongruent blocks we expect faster reaction times for incongruent items and the opposite in mainly incongruent blocks. In the remaining four blocks we did a similar manipulation to assess **reactive control**. This time not on the block-level, but the item level. Namely e.g. larger number pairs were mostly congurent and smaller number pairs were mostly incongruent.

*Diagnostic and inducer Items*: A concern in the literature is that instead of employing cognitive resources, participants simply learn to associate the specific items with the respective congruency proportion - which in short would be a completely different mechanism. To avoid learning as a confound we used two different sets of items: **Inducer items** whose proportions in congruency (80/20) where manipulated systematically to induce adaptive control and **diagnostic items** which were presented equally often congruent and incongruent. Most of the items presented to the participants were inducer items (I think, somewehere along the lines of 70/30). In total we have about 40 items per participant, per condition (i.e. proactiveControl - mainly congruent block - congrutent items, proactiveControl - mainly congruent block - incongrutent items.. and so forth). In the analysis only the diagnostic items will be considered

*Hypotheses*: We are interested whether both proactive and reactive control are present in the Parkinsons and healthy control group (I personally believe so, although the literature on that is mixed). Hence we are interested in the interaction between congruency and block proactive control, as well as the interaction congruency and block reactive control. Secondly if we can identify the interactions we are interested if ther is a group difference between these interaction effects i.e. whether Parkinson patients show some kind of impairment to healthy controls.

### Analysis Plan

In theory both reaction time and accuracy data of our participants could be analyzed - however, the task was actually so simple that participants barely made any mistakes (ceiling effect). hence we focus only on the reaction times. As reaction times are not normally distributed use a mixed effect model with a log-normal distribution using brms. We have the following factors we are interested in:

*Fixed Effects*

* **Congruency** (congruent/incongruent)
* **Group** (Parkinson/Control)
* **BlockType** Proactive Control (mainly congruent/mainly incongruent)
* **ItemType** Reactive Control (mainly congruent/mainly incongruent)

and we are interested for each group in the interaction $Congruency*BlockType$ and $Congruency*ItemType$. And the difference in both for each group.

*Random Effects*

* Participant 
* Items 

My initial idea would be to perform two separate analyses for each effect (proactive and reactive control respectively). With contrasts for each group and interaction separately (see later in the file). For the random effects I currently considered only random intercepts. 
d
Although we do not use the inducer items for the analysis, I feel like it is a waste of data not to consider them at all. Fixed effects like Congruency, and Group, as well as random effects such as item and participant should not very depending on whether we use the diagnostic or inducer items. Also I would not expect larger effects of proactive or reactive control in the diagnostic items compared to the inducer items. As such an analysis of the inducer items would give use a good indication for an upper limit of the control effects.

Based on these considerations I want to use a Bayesian mixed effects regression which I first use to fit the inducer data and whose estimates I use to create **informed priors** (for my fixed effects Congruency, group and random effects item and participant) or **regularizing priors** (Block_type and interaction effects - use the upperbound estimate for a prior in both directions). The informed priors I will use in a second step to fit the diagnostic item data

In sum

1. Identify a set of plausible priors for the analysis of the inducer items (see below) 

2. Fit the model to the inducer items

3. Use model estimates for **informed priors** on factor group, congruency, random effects and **regularizing priors** for the main effect of block/itemType and both interaction effects

4. For inference use Bayes Factors and BridgeSampling for the Main and interaction effects. As the difference between two interaction effect cannot be coded with contrasts I would use Rope/Equvalence testing to assess evidence for the null hypothesis of no difference.



## Questions I have

1. Is my contrast design (see further ahead) appropriate with regard to the questions I have in mind?

2. Is it maybe more sensible to analyze both proactive an reactive interaction effects in one model, instead of two models? 

3. To calculate the marginal median effects for my interaction am I correct in the assumption that I have to include the main effects? (I have an example written out further ahead)

4. not a question, but thanks so much already!

## Generate data
Before we look at the data, let us generate some data. Next slide is just for illustrative purposes.



The Rest of this Rmd file is just me dabling with code, generating some data and sorting my ideas. Apologies for the mess beforehand! FYI since brms codes take quite a while to run they are not evaluated automatically.
```{r}
set.seed(4298)
Sample_size<- 1112
artif_dat <- tibble(rt = exp(rtnorm(Sample_size/2, 6.5, 0.5, a = 0))) %>%
  mutate(congruency = "congruent") %>%
  bind_rows(
    tibble(
      rt = exp(rtnorm(Sample_size/2, 6.8, 0.5, a = 0)),
      congruency = "incongruent"
    )
  )
ggplot(data = artif_dat, aes(rt,  fill = congruency)) +
  geom_density(aes(group = congruency), alpha = 0.5) +
  scale_fill_brewer(palette="Set3") +
  xlab("Reaction Time") + ggtitle("Distribution Fake Reaction Time") +
  theme()


```


We assume the following priors:

$$ \alpha \sim Normal(6,1.5) $$
$$ \sigma \sim Normal_+ (0,1)$$
First we simulate the priors we slected to generate some data. We need to make some assumptions for our priors. For the numerical Stroop task we expect baseline eman RT around 500 - 600ms in healthy young participants. 

Cohen Kadosh, R., Gevers, W., & Notebaert, W. (2011). Sequential analysis of the numerical Stroop effect reveals response suppression. Journal of Experimental Psychology: Learning, Memory, and Cognition, 37(5), 1243.

Since our participants are a little older we have to adjust for age - putting our estimates more in the vicinity of about 600 - 800ms mean reaction time. Since participants have to respond within 2 seconds we have an upper bound of 2000ms. Responses under 200ms can safely be considered early random - so a lower bound of 200ms. Mean RTs of 300ms and lower can also be considered as very unlikely/improbable.

Kaufmann, L., Ischebeck, A., Weiss, E., Koppelstaetter, F., Siedentopf, C., Vogel, S. E., ... & Wood, G. (2008). An fMRI study of the numerical Stroop task in individuals with and without minimal cognitive impairment. cortex, 44(9), 1248-1255.

Effects of congruency from the previous paper range from 20ms to 100ms mean delay. Effects larger than 300ms seem very unlikely and we will assume this as an upperbound.

With regard to the effects of reactive and proactive control we look at Gonthier et al. For proactive control they reported a mean difference in Stroop effects of ~50ms in the inducer and ~30ms in the diagnostic items. Mind you, this was a "regular" Stroop task with images of animals, not a numerical stroop task. With regard to reactive control we see only a difference of 20ms in the inducer items (!). Results for the diagnostic items are not even reported. However, based on these results we should most certainly no exclude the null and we can surely disregard effects with mean magnitudes exceed 100ms. With regard to our random effects it is hard to put it into numbers but we might expect variability of plus/minus 300ms on average, upper bound of 500ms? We can use the same magnitudes as priors for the item specific effects.

With regard to group we assume a difference of 100ms for healthy controls and Parkinson's Patients

## Parameter

With these rough estimates in mind, let us first produce one dataset. First we specify our parameter for the experiment (we will do this examplary only for the proactive control for now):

```{r}
Total_trl <- 1024
Task_trl <- Total_trl/2
Block_trl <- Task_trl/2
Item_nr <- 8 # we have 8 items
sub_group <- 30
groups <- 2
n_observations <- 16

iter <- 1
grand_mu <- rnorm(iter, 6, 1)
grand_sigma <- rnorm(iter, 0, 1)
congruency_mu <- rnorm(iter, 0, 0.5)
group_mu      <- rnorm(iter, 0, 0.5)
block_mu <- rnorm(iter, 0, 0.5)


```



## Function to generate samples

Awkwardly written for-loops
```{r}
NumericalStroop_rt_fun <- function(n_observations, grand_mu, grand_sigma, congruency_mu, block_mu, group_mu){
  df_predicted <- tibble(
    rt_pred = numeric(0),
    subj    = numeric(0),
    item    = numeric(0),
    group   = numeric(0),
    congruency = numeric(0),
    Block_type = numeric(0),
    iter    = numeric(0)
  )
  for (i in seq_along(grand_mu)){
    random_int_item <- rnorm(8,0, 1)
    for (grp in seq_len(2)){
      random_int_subj <- rnorm(30,0, 1)
      for (sub in seq_len(30)){
        for (con in seq_len(2)){ # congruent/incongruent
          for(blck in seq_len(2)){
            for (itm in seq_len(8)){
              mu <- grand_mu[i]
              sigma <-grand_sigma[i]
              cong <- (con-1)*congruency_mu[i]
              group <- (grp-1)* group_mu[i]
              block <- (blck -1)*block_mu[i]
              total_mu <- mu + group + block + cong + random_int_subj[sub] + random_int_item[itm] 
              df_predicted <- bind_rows(
                df_predicted,
                tibble(rt_pred = exp(rnorm(n_observations, total_mu, sigma)),
                       subj = sub,
                       group = grp,
                       Block_type = blck,
                       congruency = con,
                       item = itm,
                       iter = i)
              )
            }
          }
        }
      }
    }
  }
  df_predicted
}

```
Now let us try this with the purr package

```{r}
# First thing we are going to do is create the basic layout of our dataset
data_layout <- function(n_observations){
  df_predicted <- tibble(
    rt_pred = numeric(0),
    subj    = numeric(0),
    item    = numeric(0),
    group   = numeric(0),
    congruency = numeric(0),
    Block_type = numeric(0),
    iter    = numeric(0)
  )
  
  random_int_item <- rnorm(8,0, 1)
  for (grp in seq_len(2)){
    random_int_subj <- rnorm(30,0, 1)
    for (sub in seq_len(30)){
      if (grp == 2){
        sub <- sub + 30
      }
      for (con in seq_len(2)){ # congruent/incongruent
        for(blck in seq_len(2)){
          for (itm in seq_len(8)){
            df_predicted <- bind_rows(
              df_predicted,
              tibble(rt_pred = replicate(n_observations, 0),
                     subj = sub,
                     group = grp-1,
                     Block_type = blck-1,
                     congruency = con-1,
                     item = itm)
            )
          }
        }
      }
    }
  } 
  df_predicted
}

layout_data_set <- data_layout(n_observations = 16)
```
Next we write a function to generate data using the datamatrix we previously created

```{r}
## Function to calculate the grand mean as a sum of the model parameters
grand_mu_calc <- function(data, grand_mu, grand_sigma, subject_mu, item_mu, congruency_mu, block_mu, group_mu){
  new_mu <- grand_mu + data$group*group_mu + data$Block_type*block_mu + data$congruency*congruency_mu + subject_mu[data$subj] + item_mu[data$item]
new_mu
}

lognormal_prior_predictive_distributions <- function(dist_params, data){
  pmap_dfr(dist_params, function(mus, sigmas, cons, blcks, groups){
  subject_mu <- rtnorm(60, 0, 0.5, a = 0)
  item_mu <- rtnorm(8, 0, 0.5, a = 0)
  grand_mu <- grand_mu_calc(
    data = data,
    grand_mu = mus,
    grand_sigma = sigmas,
    subject_mu = subject_mu,
    item_mu = item_mu,
    congruency_mu = cons,
    block_mu =  blcks,
    group_mu = groups)
  tibble( 
    rt_pred = exp(rnorm(nrow(data), grand_mu, grand_sigma)),
    grand_mu = grand_mu,
    grand_sigma = sigmas,
    subj = data$subj,
    item = data$item,
    group = data$group,
    congruency = data$congruency,
    Block_type = data$Block_type)
}, .id = "iter") %>%
  mutate(iter = as.numeric(iter))
}
```


Now that we have the function to generate data, let us try our hand at
```{r}
# First generate sample from the specified prior
iter <- 10
n_observations <- 16
grand_mu <- rnorm(iter, 6, 1)
grand_sigma <- rtnorm(iter, 0, 0.8, a = 0)
congruency_mu <- rtnorm(iter, 0, 1, a = 0)
group_mu      <- rtnorm(iter, 0, 1, a = 0)
block_mu <- rtnorm(iter, 0, 1, a = 0)
data <- layout_data_set
params <- list(mus = grand_mu, sigmas = grand_sigma, cons = congruency_mu, blcks = block_mu, groups = group_mu)

tic()
prior_predictive_samples_new <- lognormal_prior_predictive_distributions(dist_params = params,
                                                                         data=layout_data_set)
toc()
```

Now let us plot one resulting distribution

```{r}
 prior_predictive_samples_new %>% 
  filter(iter <= 18) %>%
  mutate(congruency = as_factor(congruency)) %>%
  ggplot(aes(rt_pred)) +
   geom_histogram( alpha = 0.5) +
  scale_fill_brewer(palette="Set3") +
  xlab("Reaction Time") + ggtitle("Distribution Simulated Reaction Time") +
  theme() + xlim(c(0, 5000)) + facet_wrap(~iter, ncol = 3)

```
From just looking at one plot, I get the feeling that the overall approach looks reasonable, but the individual estimates likely overestimate the actual effect size. Next we plot summary statistics for all datasets:

```{r}

sum_vars <- c("rt_pred")
prior_predictive_samples_new %>%
  group_by(iter) %>%
  summarise(median_RT = median(rt_pred), mean_RT = mean(rt_pred), min_RT = min(rt_pred), max_RT = max(rt_pred)) %>%
  select(median_RT,  mean_RT, min_RT) %>%
  pivot_longer(., cols = c(median_RT,  mean_RT, min_RT), names_to = "measure", values_to = "rt") %>%
  ggplot(aes(rt)) +
  geom_histogram() +
  facet_grid(rows = vars(measure)) + xlim(c(0, 10000))


```
So it does sort of seam like most estimates are in the right ball park in terms of median and mean. Max values unfortunately are off the chart. Ideally I would want smaller estimates - however on the log scale - I find it quite hard to set regularizing priors - as these depend on other parameter in my sample. Hence I will stop here for now and define my model.


# Defining Contrasts

Next we define the contrasts that we would like to analyze using the Hypr package. We will exclude the group factor so that we get individual interactions for each group and are better able to interpret the tripple interaction effect.
```{r, include=FALSE}
StroopCon <- hypr(
  Con_CO = (Bcon_C_CO + Binc_C_CO)/2 ~ (Bcon_I_CO + Binc_I_CO)/2, # main effect Congruence healthy controls
  Con_PD = (Bcon_C_PD + Binc_C_PD)/2 ~ (Bcon_I_PD + Binc_I_PD)/2, # main effect Congruence patients with Parkinson
  Blc_CO = (Bcon_C_CO + Bcon_I_CO)/2 ~ (Binc_C_CO + Binc_I_CO)/2, # main effect Block healthy controls
  Blc_PD = (Bcon_C_PD + Bcon_I_PD)/2 ~ (Binc_C_PD + Binc_I_PD)/2, # main effect Block patients with Parkinson
  Blc_Con_CO = (Bcon_C_CO - Bcon_I_CO)/2 ~ (Binc_C_CO - Binc_I_CO)/2, # interaction Block Congruence healthy controls
  Blc_Con_PD = (Bcon_C_PD - Bcon_I_PD)/2 ~ (Binc_C_PD - Binc_I_PD)/2, # interaction Block Congruence patients with P´arkinson
  #Grp_Blc_Con = (Bcon_C_CO - Bcon_I_CO - Binc_C_CO + Binc_I_CO) ~ (Bcon_C_PD - Bcon_I_PD - Binc_C_PD + Binc_I_PD), # this one is not independent thus included
  levels = c("Bcon_C_CO", "Bcon_I_CO", "Binc_C_CO", "Binc_I_CO", "Bcon_C_PD", "Bcon_I_PD", "Binc_C_PD", "Binc_I_PD")
)
StroopCon
```

Okay, so we have the hypotheses we want to test nailed down - next we want to set up the model and analyzed some bit of simulated data to see if our model can identify the parameter we are interested. First we have to generate the data (use a bit more constrained priors), and generate the factor which we use to generate the hypothesis matrix.

```{r}
set.seed(12223)

iter <- 1
n_observations <- 16
grand_mu <- rnorm(iter, 5.5, 1)
grand_sigma <- rtnorm(iter, 0, 0.5, a = 0)
congruency_mu <- rtnorm(iter, 0.2, 0.1, a = 0)
group_mu      <- rtnorm(iter, 0.3, 0.1, a = 0)
block_mu <- rtnorm(iter, 0.1, 0.1, a = 0)
data <- layout_data_set
params <- list(mus = grand_mu, sigmas = grand_sigma, cons = congruency_mu, blcks = block_mu, groups = group_mu)

tic()
prior_predictive_samples_new <- lognormal_prior_predictive_distributions(dist_params = params,
                                                                         data=layout_data_set)
toc()

# generate contrast factor
prior_predictive_dataset <- prior_predictive_samples_new %>%
  mutate(Contrast_F = case_when(
    group == 0 & congruency == 0 & Block_type == 0 ~ "Bcon_C_CO",
    group == 0 & congruency == 0 & Block_type == 1 ~ "Binc_C_CO",
    group == 0 & congruency == 1 & Block_type == 0 ~ "Bcon_I_CO",
    group == 0 & congruency == 1 & Block_type == 1 ~ "Binc_I_CO",
    group == 1 & congruency == 0 & Block_type == 0 ~ "Bcon_C_PD",
    group == 1 & congruency == 0 & Block_type == 1 ~ "Binc_C_PD",
    group == 1 & congruency == 1 & Block_type == 0 ~ "Bcon_I_PD",
    group == 1 & congruency == 1 & Block_type == 1 ~ "Binc_I_PD"
    )) %>%
  mutate(Contrast_F  = as_factor(Contrast_F)) 

# we have to create the contrast matrix first - ordering of levels for hypr must correspond to the ordering in the levels of the factor 
StroopCon <- hypr(
  Con_CO = (Bcon_C_CO + Binc_C_CO)/2 ~ (Bcon_I_CO + Binc_I_CO)/2, # main effect Congruence healthy controls
  Con_PD = (Bcon_C_PD + Binc_C_PD)/2 ~ (Bcon_I_PD + Binc_I_PD)/2, # main effect Congruence patients with Parkinson
  Blc_CO = (Bcon_C_CO + Bcon_I_CO)/2 ~ (Binc_C_CO + Binc_I_CO)/2, # main effect Block healthy controls
  Blc_PD = (Bcon_C_PD + Bcon_I_PD)/2 ~ (Binc_C_PD + Binc_I_PD)/2, # main effect Block patients with Parkinson
  Blc_Con_CO = (Bcon_C_CO - Bcon_I_CO)/2 ~ (Binc_C_CO - Binc_I_CO)/2, # interaction Block Congruence healthy controls
  Blc_Con_PD = (Bcon_C_PD - Bcon_I_PD)/2 ~ (Binc_C_PD - Binc_I_PD)/2, # interaction Block Congruence patients with Parkinson
  #Grp_Blc_Con = (Bcon_C_CO - Bcon_I_CO - Binc_C_CO + Binc_I_CO) ~ (Bcon_C_PD - Bcon_I_PD - Binc_C_PD + Binc_I_PD), # this one is not independent thus included
  levels = levels(prior_predictive_dataset$Contrast_F)
)
StroopCon

# assign the generated contrast matrix to our new factor
contrasts(prior_predictive_dataset$Contrast_F) <- contr.hypothesis(StroopCon)

contrasts(prior_predictive_dataset$Contrast_F)    
```

So This does not seem to be working as intended. My Prior seems to be allow for quite a bit of variance. Nevertheless, let us fit a model to my fake data with brms
```{r, include=FALSE, eval=FALSE}
fit_artif_dat <- brm( rt_pred ~ 1 + Contrast_F + (1|subj) + (1|item),
                      data = prior_predictive_dataset,
                     family = lognormal(),
                     prior = c(
                       prior(normal(6, 1.5), class = Intercept),
                       prior(normal(0,1), class = sigma),
                       prior(normal(0,1), class = b)
                     )
                     )
save(fit_artif_dat, file = "fit_artif_dat.rda")
```

Next get a summary of the parameter:
```{r, eval=FALSE}
load(file = "fit_artif_dat.rda")
fit_artif_dat
```
Let us see whether the predicted data fits the "real data" well.

```{r, eval=FALSE}
pp_check(fit_artif_dat, ndraws = 100) 
```
Check whether the predicted minimum fits well with the observed minima, maxima and median:

```{r, eval=FALSE}
pp_check(fit_artif_dat, type = "stat", stat = "min")
pp_check(fit_artif_dat, type = "stat", stat = "max")
pp_check(fit_artif_dat, type = "stat", stat = "median")
```

Plot the parameter distributions
```{r, eval=FALSE}
mcmc_dens(fit_artif_dat, pars = variables(fit_artif_dat)[1:10])
```
Posterior Variable fit
```{r, include = FALSE, eval=FALSE}
posterior_summary(fit_artif_dat)
```
Next let us calculate the median effects in milliseconds
```{r, eval=FALSE}
alpha_samples <- as_draws_df(fit_artif_dat)$b_Intercept
congruence_CO <- as_draws_df(fit_artif_dat)$b_Contrast_FCon_CO 
congruence_PD <- as_draws_df(fit_artif_dat)$b_Contrast_FCon_PD 
Block_CO      <- as_draws_df(fit_artif_dat)$b_Contrast_FBlc_CO
Block_PD      <- as_draws_df(fit_artif_dat)$b_Contrast_FBlc_PD
ConBlock_CO   <- as_draws_df(fit_artif_dat)$b_Contrast_FBlc_Con_CO
ConBlock_PD   <- as_draws_df(fit_artif_dat)$b_Contrast_FBlc_Con_PD


congruence_ms_CO <-  exp(alpha_samples-0.5*congruence_CO) - exp(alpha_samples+0.5*congruence_CO)
congruence_ms_PD <-  exp(alpha_samples-0.5*congruence_PD) - exp(alpha_samples+0.5*congruence_PD)
block_ms_CO      <-  exp(alpha_samples-0.5*Block_CO) - exp(alpha_samples+0.5*Block_CO)
block_ms_PD      <-  exp(alpha_samples-0.5*Block_PD) - exp(alpha_samples+0.5*Block_PD)
ConBlock_CO      <-  exp(alpha_samples-0.5*ConBlock_CO) - exp(alpha_samples+0.5*ConBlock_CO)
ConBlock_PD      <-  exp(alpha_samples-0.5*ConBlock_PD) - exp(alpha_samples+0.5*ConBlock_PD)
Diff_int         <-  ConBlock_CO - ConBlock_PD # Median difference in the interaction effects

ggplot(data = tibble(rt =congruence_ms_CO), aes(rt)) +
  geom_density() + xlab("Time in ms") + ggtitle("Mean Difference Congruence_Old Reaction Time")
c(median_rt_diff = mean(congruence_ms_CO), quantile(
  congruence_ms_CO, c(0.025, 0.975)))

ggplot(data = tibble(rt =Diff_int), aes(rt)) +
  geom_density() + xlab("Time in ms") + ggtitle("Mean Difference Interaction Reaction Time")
c(median_rt_diff = mean(Diff_int), quantile(Diff_int, c(0.025, 0.975)))

```
Although we did get some results for the marginal interaction effects in milli-seconds (or rather I calculated them by hand), I feel that they are wrong. I think it is not possible to calculate them without taking the interaction effects into consideration. In my eyes it might be more appropriate to calculate the marginal means for each possible condition e.g.: 
$$CongruentBlock1CO = \alpha + \beta_{congruenceCO}*-0.5 +\beta_{blockCO}*-0.5 + \beta_ {interactionCO}*-0.5$$
$$IncongruentBlock1CO = \alpha + \beta_{congruenceCO}*0.5 +\beta_{blockCO}*-0.5 + \beta_ {interactionCO}*-0.5$$
$$CongruentBlock2CO = \alpha + \beta_{congruenceCO}*-0.5 +\beta_{blockCO}*0.5 + \beta_ {interactionCO}*0.5$$
$$IncongruentBlock2CO = \alpha + \beta_{congruenceCO}*0.5 +\beta_{blockCO}*0.5 + \beta_ {interactionCO}*0.5$$
The my marginal medianan interaction effect would be:

$$ MM_{Interaction} = (CongruentBlock1CO -IncongruentBlock1CO) - (CongruentBlock2CO -IncongruentBlock2CO) $$
Same goes for the PD group and only the could I interpret the difference as the difference in interaction effects.

# Integrate Posterior Information in the priors of the next model
For the next model we have to somehow integrate information from the posterior, into the prior of the next model. In order to properly do that we have to define a prior for each individual contrast. To understand how to do that we first get a summary of the priors we used in the previous model and now make them explicit in the second model.

```{r, eval=FALSE}
# get model parameters used before and naming conventions
prior_summary(fit_artif_dat)

# write down priors
prior_informed <- c(
  prior(normal(6, 1.5), class = Intercept),
                       prior(normal(0,1), class = sigma),
                       prior(normal(0,1), class = b, coef = Contrast_FBlc_CO),
                       prior(normal(0,1), class = b, coef = Contrast_FBlc_PD),
                       prior(normal(0,1), class = b, coef = Contrast_FCon_CO),
                       prior(normal(0,1), class = b, coef = Contrast_FCon_PD),
                       prior(normal(0,1), class = b, coef = Contrast_FBlc_Con_CO),
                       prior(normal(0,1), class = b, coef = Contrast_FBlc_Con_PD),
                       prior(normal(0, 3), class = sd, coef = Intercept, group = subj),
                       prior(normal(0, 20), class = sd, coef = Intercept, group = item)
)

informed_artif_dat <- brm( rt_pred ~ 1 + Contrast_F + (1|subj) + (1|item),
                      data = prior_predictive_dataset,
                     family = lognormal(),
                     prior = prior_informed,
                     warmup = 2000,
                     iter = 20000, # 20000 is the limit necessary for bridge sampling
                     save_pars = save_pars(all = TRUE), # must be set to true for bridgesampling
                     )
save(informed_artif_dat, file = "fit_artif_dat.rda")

# to calculate the bayes factor use the following syntax:
margLogLik_mod1 <- bridge_sampler(fit_N400_h_linear, silent = TRUE)
margLogLik_mod2 <- bridge_sampler(fit_N400_h_null, silent = TRUE)
# the calculate the Bayesfactor
BF_ln <- bayes_factor(margLogLik_mod1, margLogLik_mod2)
# Can also be computed by hand, first calculate the difference of the log-liklihoods (same as division), then exponentiate the differences 



```
## Note to self: at this point it would probably make sense to consider if we want include random slopes for our effects for each participant and perhaps subjects and items


## Notes for in between

For the model we only have to estimate the parameter for the partial pooling parameter - Hence I do not need an estimate for every single participant - but instead can use the estimate I get from my Inducer parameter. I will make it not 100% informative but only use it to restrict the range of parameter